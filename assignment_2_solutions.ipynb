{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from time import time\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALL major classifiers are imported below\n",
    "\n",
    "##### USE ANY 2 of the below classifiers, Make sure to understand the documentation and parameters before using them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USE ANY OF THE BELOW VECTORIZERS, Again read the documentation to understand the parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### USE EITHER GRIDSEARCHCV or RANDOMSEARCHCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Use the below function for benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(clf_class, params, name= 'classifier'):\n",
    "    print(\"parameters:\", params)\n",
    "    t0 = time()\n",
    "    clf = clf_class(**params).fit(X_train, y_train)\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "    print(\"Predicting the outcomes of the testing set\")\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "    \n",
    "    print (\" Printing Accuracy of the classifier\")\n",
    "    print (accuracy_score(y_test, pred))\n",
    "    print ()\n",
    "\n",
    "    print(\"Classification report on test set for classifier:\")\n",
    "    print(clf)\n",
    "    print()\n",
    "    print(classification_report(y_test, pred, target_names=categories))\n",
    "    \n",
    "    print ()\n",
    "    print (\"FI score metrics\")\n",
    "    print (precision_recall_fscore_support(y_test, pred, average='weighted'))\n",
    "\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Show confusion matrix\n",
    "    plt.matshow(cm)\n",
    "    plt.title('CM of the %s ' % name)\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and choose 10 categories\n",
    "\n",
    "categories_all = ['alt.atheism',\n",
    " 'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey',\n",
    " 'sci.crypt',\n",
    " 'sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.politics.misc',\n",
    " 'talk.religion.misc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting 10 random categories\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(categories_all) # Randomly shuffle the categories, pick any 10\n",
    "categories = [categories_all.pop() for i in range(10)]\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, \n",
    "                                  remove=('headers', 'footers', 'quotes'), categories= categories)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True, remove=('headers', 'footers', 'quotes'),\n",
    "                                 categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets use TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5713, 2553)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = TfidfVectorizer(stop_words = {'english'}, lowercase=True, analyzer ='word', binary =False,min_df = 0.005)\n",
    "\n",
    "X_train = vec.fit_transform(twenty_train.data)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3806, 2553)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = vec.transform(twenty_test.data)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = twenty_train.target\n",
    "y_test = twenty_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyMultiNomialNB(X_train, y_train):\n",
    "\tclf = MultinomialNB()\n",
    "\tparam_grid = {'alpha': [0.01,0.1, 1.0, 10, 100]}\n",
    "\t# Five fold Cross Validation\n",
    "\tclassifier= GridSearchCV(estimator=clf, cv=3 ,param_grid=param_grid)\n",
    "\tclassifier.fit(X_train, y_train)\n",
    "\treturn classifier.cv_results_, classifier.best_estimator_, classifier.best_params_, classifier.best_score_\n",
    "\n",
    "\n",
    "def Mylinear_svm(X_train, y_train):\n",
    "\tlinear_svc = svm.SVC(kernel='linear')\n",
    "\tparam_grid = {'C': np.logspace(-3, 2, 6)} # C is the regularizer, widens or shortens the width of the hyper plane\n",
    "\tclassifier= GridSearchCV(estimator=linear_svc, cv=3 ,param_grid=param_grid)\n",
    "\ty_train= np.array(y_train)\n",
    "\tclassifier.fit(X_train, y_train)\n",
    "\treturn classifier.cv_results_, classifier.best_estimator_, classifier.best_params_, classifier.best_score_\n",
    "\n",
    "\n",
    "def MyRandomForest(X_train, y_train):\n",
    "\tclf = RandomForestClassifier()\n",
    "\tparam_grid = {'n_estimators': [10,20,30,50,70,100],'max_depth': [5,10,20]}\n",
    "\tclassifier= GridSearchCV(estimator=clf, cv=3 ,param_grid=param_grid)\n",
    "\tclassifier.fit(X_train, y_train)\n",
    "\treturn classifier.cv_results_, classifier.best_estimator_, classifier.best_params_, classifier.best_score_\n",
    "\n",
    "def MyDecisionTree(X_train, y_train):\n",
    "\tclf = DecisionTreeClassifier(min_samples_split=2,random_state=0)\n",
    "\tparam_grid = {'max_depth': [1,5,10,25,50, 100, 200]}\n",
    "\tclassifier= GridSearchCV(estimator=clf, cv=3 ,param_grid=param_grid)\n",
    "\tclassifier.fit(X_train, y_train)\n",
    "\treturn classifier.cv_results_, classifier.best_estimator_, classifier.best_params_, classifier.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results, best_estimator, best_params_rf, best_score = MyRandomForest(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " best estimator is {'max_depth': 20, 'n_estimators': 100} and score is 0.6943812357780501\n"
     ]
    }
   ],
   "source": [
    "print (f\" best estimator is {best_params_rf} and score is {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decison Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results, best_estimator, best_params_dt, best_score = MyDecisionTree(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " best estimator is {'max_depth': 200} and score is 0.5307194118676702\n"
     ]
    }
   ],
   "source": [
    "print (f\" best estimator is {best_params_dt} and score is {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM ( Takes a long time, skip for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.e-03 1.e-02 1.e-01 1.e+00 1.e+01 1.e+02]\n"
     ]
    }
   ],
   "source": [
    "#cv_results, best_estimator, best_params_svm, best_score = Mylinear_svm(X_train, y_train)\n",
    "print (np.logspace(-3, 2, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (f\" best estimator is {best_params_svm} and score is {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiNomial Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results, best_estimator, best_params_nb, best_score = MyMultiNomialNB(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " best estimator is {'alpha': 0.1} and score is 0.7638718711710135\n"
     ]
    }
   ],
   "source": [
    "print (f\" best estimator is {best_params_nb} and score is {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best accuracy for now is Multinomial NB, You can call the benchmark function to evaluate on test and get the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: {'alpha': 0.1}\n",
      "done in 0.017948s\n",
      "Predicting the outcomes of the testing set\n",
      "done in 0.003120s\n",
      " Printing Accuracy of the classifier\n",
      "0.7377824487651077\n",
      "\n",
      "Classification report on test set for classifier:\n",
      "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)\n",
      "\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "     talk.politics.guns       0.62      0.65      0.63       319\n",
      "              rec.autos       0.69      0.70      0.70       389\n",
      "           misc.forsale       0.74      0.69      0.71       394\n",
      "                sci.med       0.76      0.71      0.73       385\n",
      "     rec.sport.baseball       0.83      0.80      0.81       390\n",
      "  comp.sys.mac.hardware       0.71      0.76      0.73       396\n",
      "comp.os.ms-windows.misc       0.69      0.86      0.76       397\n",
      "            alt.atheism       0.81      0.71      0.76       396\n",
      "  talk.politics.mideast       0.74      0.72      0.73       364\n",
      "          comp.graphics       0.82      0.76      0.79       376\n",
      "\n",
      "               accuracy                           0.74      3806\n",
      "              macro avg       0.74      0.74      0.74      3806\n",
      "           weighted avg       0.74      0.74      0.74      3806\n",
      "\n",
      "\n",
      "FI score metrics\n",
      "(0.7416677806786182, 0.7377824487651077, 0.738029506889057, None)\n",
      "Confusion matrix:\n",
      "[[206   8   2   0   0  14  17  20  24  28]\n",
      " [ 10 273  40  24  14  11  10   5   1   1]\n",
      " [  9  45 272  28   3   6  17   5   8   1]\n",
      " [  1  19  34 275  20  13  15   5   2   1]\n",
      " [  2   4   6  25 312  18  14   2   5   2]\n",
      " [ 11   7   4   6  11 299  28  12  14   4]\n",
      " [ 10   7   3   2   9   9 341   5   8   3]\n",
      " [ 17  26   2   2   5  24  21 283  10   6]\n",
      " [ 29   1   4   0   2  21  20   8 262  17]\n",
      " [ 38   3   3   2   1   7  13   4  20 285]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "benchmark(MultinomialNB,best_params_nb )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks decent and results could be improved with nlp techniques such as proper preprocessing, stop words removal\n",
    "or other algos. But this is okay for our experiements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For just one classifier with one vectorizer, the performance of GridSearch and RandomSearch are nearly the same.\n",
    "\n",
    "### But there are many other classifiers with their own parameters ?\n",
    "### We could even use a different vectorizer ( count, tf-idf, hashing with its own parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE ANSWER IS BUILDING A PIPELINE AND DO A GRIDSEARCH ON THEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple example pipeline to to a classifier, its parameters, vectorizer and GridSearchCV\n",
    "##### lets do this again with 4 categories for time saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2 categories and get train and test data\n",
    "\n",
    "categories = ['alt.atheism', 'rec.sport.baseball']\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, \n",
    "                                  remove=('headers', 'footers', 'quotes'), categories= categories)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True, remove=('headers', 'footers', 'quotes'),\n",
    "                                 categories=categories)\n",
    "\n",
    "y_train = twenty_train.target\n",
    "y_test = twenty_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets initialize a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DummyEstimator(BaseEstimator):\n",
    "    def fit(self): pass\n",
    "    def score(self): pass\n",
    "\n",
    "class DummyTransformer(TransformerMixin):\n",
    "    def fit_transform(self, X, y=None, **fit_params): pass\n",
    "    def fit(self): pass\n",
    "    def tranform(self): pass\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', DummyEstimator()),\n",
    "])\n",
    "\n",
    "# A fit transform followed by an estimator\n",
    "\n",
    "search_space = [\n",
    "    {\n",
    "        'vect': (TfidfVectorizer(),),\n",
    "        'vect__max_df': (0.5, 0.75),\n",
    "        'clf': (LogisticRegression(),),\n",
    "        'clf__C': ( 0.1, 1, 10),\n",
    "        'clf__penalty': ('l2', 'l1'),\n",
    "    }, \n",
    "    {\n",
    "        'vect': (TfidfVectorizer(),),\n",
    "        'vect__max_df': (0.5, 0.75),\n",
    "        'clf': (RandomForestClassifier(),),\n",
    "        'clf__n_estimators': (10,20,30,50),\n",
    "        'clf__max_depth': (5,10,20),\n",
    "    },\n",
    "    {\n",
    "        'vect': (TfidfVectorizer(),),\n",
    "        'vect__max_df': (0.5, 0.75),\n",
    "        'clf': (DecisionTreeClassifier(),),\n",
    "        'clf__max_depth': (1,5,10,25,50),\n",
    "    }, \n",
    "    {\n",
    "        'vect': (TfidfVectorizer(),),\n",
    "        'vect__max_df': (0.5, 0.75),\n",
    "        'clf': (MultinomialNB(),),\n",
    "        'clf__alpha': (0.01,0.1, 1.0, 10, 100),\n",
    "    }   \n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(pipeline, search_space, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 56 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Done 280 out of 280 | elapsed:   55.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words...\n",
       "                                                   lowercase=True, max_df=0.5,\n",
       "                                                   max_features=None, min_df=1,\n",
       "                                                   ngram_range=(1, 1),\n",
       "                                                   norm='l2', preprocessor=None,\n",
       "                                                   smooth_idf=True,\n",
       "                                                   stop_words=None,\n",
       "                                                   strip_accents=None,\n",
       "                                                   sublinear_tf=False,\n",
       "                                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                   tokenizer=None, use_idf=True,\n",
       "                                                   vocabulary=None),),\n",
       "                          'vect__max_df': (0.5, 0.75)}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " best estimator is {'clf': MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True), 'clf__alpha': 0.01, 'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=0.5, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None), 'vect__max_df': 0.5} and score is 0.9591457753017641\n"
     ]
    }
   ],
   "source": [
    "gs_clf.best_params_\n",
    "# {'clf__C': 0.1}\n",
    "\n",
    "gs_clf.best_score_\n",
    "# 0.702290076336\n",
    "\n",
    "print (f\" best estimator is {gs_clf.best_params_} and score is {gs_clf.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can add even more parameters and search your classifiers with even more vectorizers like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', DummyEstimator()),\n",
    "])\n",
    "\n",
    "# A fit transform followed by an estimator\n",
    "\n",
    "search_space = [\n",
    "    {\n",
    "        'vect': (TfidfVectorizer(),),\n",
    "        'vect__max_df': (0.5, 0.75),\n",
    "        'clf': (LogisticRegression(),),\n",
    "        'clf__C': ( 0.1, 1, 10),\n",
    "        'clf__penalty': ('l2', 'l1'),\n",
    "    }, \n",
    "    {\n",
    "        'vect': (TfidfVectorizer(),),\n",
    "        'vect__max_df': (0.5, 0.75),\n",
    "        'clf': (RandomForestClassifier(),),\n",
    "        'clf__n_estimators': (10,20,30,50),\n",
    "        'clf__max_depth': (5,10,20),\n",
    "    },\n",
    "    {\n",
    "        'vect': (TfidfVectorizer(),),\n",
    "        'vect__max_df': (0.5, 0.75),\n",
    "        'clf': (DecisionTreeClassifier(),),\n",
    "        'clf__max_depth': (1,5,10,25,50),\n",
    "    }, \n",
    "    {\n",
    "        'vect': (TfidfVectorizer(),),\n",
    "        'vect__max_df': (0.5, 0.75),\n",
    "        'clf': (MultinomialNB(),),\n",
    "        'clf__alpha': (0.01,0.1, 1.0, 10, 100),\n",
    "    },\n",
    "        {\n",
    "        'vect': (CountVectorizer(),),\n",
    "        'vect__max_df': (0.5, 0.75),\n",
    "        'clf': (LogisticRegression(),),\n",
    "        'clf__C': ( 0.1, 1, 10),\n",
    "        'clf__penalty': ('l2', 'l1'),\n",
    "    }, \n",
    "    {\n",
    "        'vect': (CountVectorizer(),),\n",
    "        'vect__max_df': (0.5, 0.75),\n",
    "        'clf': (RandomForestClassifier(),),\n",
    "        'clf__n_estimators': (10,20,30,50),\n",
    "        'clf__max_depth': (5,10,20),\n",
    "    },\n",
    "    {\n",
    "        'vect': (CountVectorizer(),),\n",
    "        'vect__max_df': (0.5, 0.75),\n",
    "        'clf': (DecisionTreeClassifier(),),\n",
    "        'clf__max_depth': (1,5,10,25,50),\n",
    "    }, \n",
    "    {\n",
    "        'vect': (CountVectorizer(),),\n",
    "        'vect__max_df': (0.5, 0.75),\n",
    "        'clf': (MultinomialNB(),),\n",
    "        'clf__alpha': (0.01,0.1, 1.0, 10, 100),\n",
    "    }\n",
    "    \n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 112 candidates, totalling 560 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " best estimator is {'clf': MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True), 'clf__alpha': 0.01, 'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=0.5, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None), 'vect__max_df': 0.5} and score is 0.9591457753017641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 560 out of 560 | elapsed:  1.8min finished\n"
     ]
    }
   ],
   "source": [
    "gs_clf = GridSearchCV(pipeline, search_space, cv=5, verbose=1)\n",
    "gs_clf.fit(twenty_train.data, twenty_train.target)\n",
    "print (f\" best estimator is {gs_clf.best_params_} and score is {gs_clf.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next More into pipelines, base estimator and transformer classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before that Practise pipeline with GridSearch on iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
