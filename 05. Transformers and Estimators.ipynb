{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Components\n",
    "In machine learning, it is common to run a sequence of algorithms to process and learn from data. E.g., a simple text document processing workflow might include several stages:\n",
    "\n",
    "Split each document’s text into words.\n",
    "Convert each document’s words into a numerical feature vector.\n",
    "Learn a prediction model using the feature vectors and labels.\n",
    "\n",
    "Let's start by understanding different components of a pipeline:\n",
    "### 1.Transformer\n",
    "\n",
    "A Transformer is an abstraction that includes feature transformers and learned models. Technically, a Transformer implements a method transform(), which converts one DataFrame into another, generally by appending one or more columns. For example:\n",
    "\n",
    "* A feature transformer might take a DataFrame, read a column (e.g., text), map it into a new column (e.g., feature vectors), and output a new DataFrame with the mapped column appended.\n",
    "* A learning model might take a DataFrame, read the column containing feature vectors, predict the label for each feature vector, and output a new DataFrame with predicted labels appended as a column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Estimator\n",
    "\n",
    "An Estimator abstracts the concept of a learning algorithm or any algorithm that fits or trains on data. Technically, an Estimator implements a method fit(), which accepts a DataFrame and produces a Model, which is a Transformer. \n",
    "\n",
    "For example, a learning algorithm such as LogisticRegression is an Estimator, and calling fit() trains a LogisticRegressionModel, which is a Model and hence a Transformer.\n",
    "\n",
    "\n",
    "### How it works\n",
    "A Pipeline is specified as a sequence of stages, and each stage is either a Transformer or an Estimator. These stages are run in order, and the input DataFrame is transformed as it passes through each stage. For Transformer stages, the transform() method is called on the DataFrame. For Estimator stages, the fit() method is called to produce a Transformer (which becomes part of the PipelineModel, or fitted Pipeline), and that Transformer’s transform() method is called on the DataFrame.\n",
    "\n",
    "We illustrate this for the simple text document workflow. The figure below is for the training time usage of a Pipeline.\n",
    "\n",
    "<img src=\"images/ml-pipeline.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps involved in the above pipeline:\n",
    "    1. Tokenization -- Converting raw text into tokenized form (Transformer)\n",
    "    2. HashingTF -- Converting words to feature vectors (Transformer)\n",
    "    3. Logistic Regression  -- Fit the data to logistic regression (Estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to make your custom transformers?\n",
    "\n",
    "What if we want to write our own transformer for the data we are dealing with? Should we create a class, with fit, fit_transform, and define all other methods? Inorder to simplify our work, we can just follow the pattern of existing transformers. \n",
    "\n",
    "Let's dig deeper into Transformer class. To dig deeper, let's have at the source code of one of the transformers\n",
    "One hot encoding: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "\n",
    "How to check the source code?\n",
    "\n",
    "<img src=\"images/transformers.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transformer class in scikit learn inherits two parent classes:\n",
    "1. BaseEstimator\n",
    "2. TransformerMixin\n",
    "\n",
    "To read the source code for the BaseEstimator class, go here: https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/base.py#L176\n",
    "\n",
    "\n",
    "To read the source code for the TransformerMixin class, go here: https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/base.py#L490"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TransformerMixin:\n",
    "Inheriting from TransformerMixin ensures that all we need to do is write our fit and transform methods and we get fit_transform for free\n",
    "\n",
    "\n",
    "BaseEstimator:\n",
    "Inheriting from BaseEstimator ensures we get get_params and set_params for free."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making our own Transformers and Estimators\n",
    "\n",
    "Sample example:\n",
    "An example of transformer that does nothing\n",
    "\n",
    "```python\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class LazyTransformer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, x, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "--------------\n",
    "\n",
    "1. Write a transformer that adds some number to the input, the number that is added should be passed in `__init__`\n",
    "2. Write a transformer that normalizes the input:\n",
    "   - in the fit method you must save the column means\n",
    "3. Combine these 2 transformers into a pipeline:\n",
    "   - hint: write a class that accepts list of transformers as argument\n",
    "   \n",
    "HINTS: All transformers are classes! All classes must have an `__init__` function. All transformers must inherit from the TransformerMixin parent class. All estimators must inherit from the BaseEstimator parent class. All transformers must have `fit` and `transform` functions (methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# answer - start\n",
    "\n",
    "from sklearn.base import TransformerMixin, BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-6-7228c0e6d8a2>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-7228c0e6d8a2>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    class MeanNormalizer(TransformerMixin):\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "## Fill in the answer in template below:\n",
    "import numpy as np\n",
    "\n",
    "# answer - start\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class AdderTransformer(TransformerMixin):\n",
    "    \n",
    "\n",
    "class MeanNormalizer(TransformerMixin): \n",
    "\n",
    "    \n",
    "class TransformerPipeline(TransformerMixin):\n",
    "\n",
    "\n",
    "# answer - end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/transformerSol.py\n",
    "class AdderTransformer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, add=0):\n",
    "        self.add = add\n",
    "        \n",
    "    def fit(self, x, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return x + self.add\n",
    "    \n",
    "class MeanNormalizer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def fit(self, x, y = None):\n",
    "        self.means = x.mean(axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x):\n",
    "        return x - self.means    \n",
    "    \n",
    "class TransformerPipeline(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, transformers):\n",
    "        self.transformers = transformers\n",
    "        \n",
    "    def fit(self, x, y = None):\n",
    "        x_ = x.copy()\n",
    "        for transformer in self.transformers:\n",
    "            transformer.fit(x_)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, x):\n",
    "        x_ = x.copy()\n",
    "        for transformer in self.transformers:\n",
    "            x_ = transformer.transform(x_)\n",
    "        return x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.ones((10,10))\n",
    "adder = AdderTransformer(add=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "       [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "       [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "       [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "       [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "       [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "       [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "       [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "       [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "       [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Add 1\n",
    "add = AdderTransformer(add = 1)\n",
    "add.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Mean normalize\n",
    "norm = MeanNormalizer()\n",
    "norm.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = [AdderTransformer(add=1), \n",
    "                MeanNormalizer()]\n",
    "pipeline = TransformerPipeline(transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit_transform(X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tying it all up together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we build a pipeline using SCikit Learn?\n",
    "Fortunately scikit-learn provides a set of helpful functions to deal with pipelines.\n",
    "2 of them are the most important:\n",
    "\n",
    "1. `sklearn.pipeline.make_pipeline`\n",
    "\n",
    "    In our previous example we could define our transformer like this\n",
    "    \n",
    "```python\n",
    "adder_normalizer = make_pipeline(\n",
    "    AdderTransformer(add=10),\n",
    "    MeanNormalizer()\n",
    ")\n",
    "```\n",
    "Calling `fit` on the pipeline is the same as calling fit on each estimator  or transformoers in turn, transform the input and pass it on to the next step. The pipeline has all the methods that the last estimator in the pipeline has, i.e. if the last estimator is a classifier, the Pipeline can be used as a classifier. If the last estimator is a transformer, again, so is the pipeline.\n",
    "\n",
    "2. `sklearn.pipeline.make_union`\n",
    "\n",
    "    Creates a union of transformers\n",
    "    \n",
    "    ```\n",
    "    \n",
    "             transformer 1\n",
    "           /               \\\n",
    "          /                 \\\n",
    "    input                     output\n",
    "          \\                 /    \n",
    "           \\               /\n",
    "             transformer 2\n",
    "             \n",
    "    ```\n",
    "             \n",
    "    It is useful when the dataset consists of several types of data that one must \n",
    "    deal with separately.\n",
    "\n",
    "\n",
    "Alternative way to define pipelines\n",
    "--------------\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "adder_normalizer = Pipeline([\n",
    "    ('adder', AdderTransformer(add=10)),\n",
    "    ('normalizer', MeanNormalizer()),    \n",
    "])\n",
    "\n",
    "print(adder_normalizer)\n",
    "\n",
    ">> Pipeline(steps=[('adder', <__main__.AdderTransformer object at 0x7f9387473750>), ('normalizer', <__main__.MeanNormalizer object at 0x7f9387137e50>)])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "    \n",
    "    1. https://spark.apache.org/docs/1.6.2/ml-guide.html#transformers\n",
    "    2. https://github.com/rachelkberryman/DSR_Model_Pipelines_Course/blob/master/1.3%20Pipelines.ipynb\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
