{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from time import time\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALL major classifiers are imported below\n",
    "\n",
    "##### USE ANY 2 of the below classifiers, Make sure to understand the documentation and parameters before using them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USE ANY OF THE BELOW VECTORIZERS, Again read the documentation to understand the parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### USE EITHER GRIDSEARCHCV or RANDOMSEARCHCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Use the below function for benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(clf_class, params, name= 'classifier'):\n",
    "    print(\"parameters:\", params)\n",
    "    t0 = time()\n",
    "    clf = clf_class(**params).fit(X_train, y_train)\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "    print(\"Predicting the outcomes of the testing set\")\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "    \n",
    "    print (\" Printing Accuracy of the classifier\")\n",
    "    print (accuracy_score(y_test, pred))\n",
    "    print ()\n",
    "\n",
    "    print(\"Classification report on test set for classifier:\")\n",
    "    print(clf)\n",
    "    print()\n",
    "    print(classification_report(y_test, pred, target_names=categories))\n",
    "    \n",
    "    print ()\n",
    "    print (\"FI score metrics\")\n",
    "    print (precision_recall_fscore_support(y_test, pred, average='weighted'))\n",
    "\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Show confusion matrix\n",
    "    plt.matshow(cm)\n",
    "    plt.title('CM of the %s ' % name)\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and choose 10 categories\n",
    "\n",
    "categories_all = ['alt.atheism',\n",
    " 'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey',\n",
    " 'sci.crypt',\n",
    " 'sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.politics.misc',\n",
    " 'talk.religion.misc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting 10 random categories\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(categories_all) # Randomly shuffle the categories, pick any 10\n",
    "categories = [categories_all.pop() for i in range(10)]\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, \n",
    "                                  remove=('headers', 'footers', 'quotes'), categories= categories)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True, remove=('headers', 'footers', 'quotes'),\n",
    "                                 categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets use TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5817, 2572)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = TfidfVectorizer(stop_words = {'english'}, lowercase=True, analyzer ='word', binary =False,min_df = 0.005)\n",
    "\n",
    "X_train = vec.fit_transform(twenty_train.data)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3872, 2572)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = vec.transform(twenty_test.data)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = twenty_train.target\n",
    "y_test = twenty_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyMultiNomialNB(X_train, y_train):\n",
    "\tclf = MultinomialNB()\n",
    "\tparam_grid = {'alpha': [0.01,0.1, 1.0, 10, 100]}\n",
    "\t# Five fold Cross Validation\n",
    "\tclassifier= GridSearchCV(estimator=clf, cv=3 ,param_grid=param_grid)\n",
    "\tclassifier.fit(X_train, y_train)\n",
    "\treturn classifier.cv_results_, classifier.best_estimator_, classifier.best_params_, classifier.best_score_\n",
    "\n",
    "\n",
    "def Mylinear_svm(X_train, y_train):\n",
    "\tlinear_svc = svm.SVC(kernel='linear')\n",
    "\tparam_grid = {'C': np.logspace(-3, 2, 6)} # C is the regularizer, widens or shortens the width of the hyper plane\n",
    "\tclassifier= GridSearchCV(estimator=linear_svc, cv=3 ,param_grid=param_grid)\n",
    "\ty_train= np.array(y_train)\n",
    "\tclassifier.fit(X_train, y_train)\n",
    "\treturn classifier.cv_results_, classifier.best_estimator_, classifier.best_params_, classifier.best_score_\n",
    "\n",
    "\n",
    "def MyRandomForest(X_train, y_train):\n",
    "\tclf = RandomForestClassifier()\n",
    "\tparam_grid = {'n_estimators': [10,20,30,50,70,100],'max_depth': [5,10,20]}\n",
    "\tclassifier= GridSearchCV(estimator=clf, cv=3 ,param_grid=param_grid)\n",
    "\tclassifier.fit(X_train, y_train)\n",
    "\treturn classifier.cv_results_, classifier.best_estimator_, classifier.best_params_, classifier.best_score_\n",
    "\n",
    "def MyDecisionTree(X_train, y_train):\n",
    "\tclf = DecisionTreeClassifier(min_samples_split=2,random_state=0)\n",
    "\tparam_grid = {'max_depth': [1,5,10,25,50, 100, 200]}\n",
    "\tclassifier= GridSearchCV(estimator=clf, cv=3 ,param_grid=param_grid)\n",
    "\tclassifier.fit(X_train, y_train)\n",
    "\treturn classifier.cv_results_, classifier.best_estimator_, classifier.best_params_, classifier.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results, best_estimator, best_params_rf, best_score = MyRandomForest(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " best estimator is {'max_depth': 20, 'n_estimators': 70} and score is 0.6618531889290012\n"
     ]
    }
   ],
   "source": [
    "print (f\" best estimator is {best_params_rf} and score is {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decison Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results, best_estimator, best_params_dt, best_score = MyDecisionTree(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " best estimator is {'max_depth': 100} and score is 0.4990544954443871\n"
     ]
    }
   ],
   "source": [
    "print (f\" best estimator is {best_params_dt} and score is {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM ( Takes a long time, skip for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.e-03 1.e-02 1.e-01 1.e+00 1.e+01 1.e+02]\n"
     ]
    }
   ],
   "source": [
    "#cv_results, best_estimator, best_params_svm, best_score = Mylinear_svm(X_train, y_train)\n",
    "print (np.logspace(-3, 2, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (f\" best estimator is {best_params_svm} and score is {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiNomial Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results, best_estimator, best_params_nb, best_score = MyMultiNomialNB(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " best estimator is {'alpha': 0.1} and score is 0.7562317345710847\n"
     ]
    }
   ],
   "source": [
    "print (f\" best estimator is {best_params_nb} and score is {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best accuracy for now is Multinomial NB, You can call the benchmark function to evaluate on test and get the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters: {'alpha': 0.1}\n",
      "done in 0.007372s\n",
      "Predicting the outcomes of the testing set\n",
      "done in 0.002978s\n",
      " Printing Accuracy of the classifier\n",
      "0.7303719008264463\n",
      "\n",
      "Classification report on test set for classifier:\n",
      "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)\n",
      "\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "               sci.space       0.71      0.71      0.71       394\n",
      " comp.os.ms-windows.misc       0.78      0.74      0.76       392\n",
      "        rec.sport.hockey       0.71      0.71      0.71       396\n",
      "                 sci.med       0.72      0.71      0.72       398\n",
      "comp.sys.ibm.pc.hardware       0.80      0.76      0.78       397\n",
      "               sci.crypt       0.64      0.84      0.73       399\n",
      "               rec.autos       0.80      0.72      0.76       396\n",
      "      rec.sport.baseball       0.80      0.72      0.76       396\n",
      "         rec.motorcycles       0.75      0.70      0.72       394\n",
      "      talk.politics.misc       0.64      0.68      0.66       310\n",
      "\n",
      "                accuracy                           0.73      3872\n",
      "               macro avg       0.73      0.73      0.73      3872\n",
      "            weighted avg       0.74      0.73      0.73      3872\n",
      "\n",
      "\n",
      "FI score metrics\n",
      "(0.7357622662095749, 0.7303719008264463, 0.7310597876464805, None)\n",
      "Confusion matrix:\n",
      "[[280  53   1   7   4  15  11  10   8   5]\n",
      " [ 63 291  15   3   2   9   6   1   2   0]\n",
      " [  4   3 280  37   4  29   5   7  17  10]\n",
      " [  2   7  46 284   7  16   2   8  12  14]\n",
      " [  7   3   4   5 301  50   4   5   2  16]\n",
      " [  3   0   6   8  34 335   4   2   2   5]\n",
      " [ 16   5   6  10   8  17 286   7  13  28]\n",
      " [  8   5  18  19   2  19   5 287  17  16]\n",
      " [ 11   4  10   9   6  22  10  22 274  26]\n",
      " [  2   2  11  13   9  10  25  10  18 210]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "benchmark(MultinomialNB,best_params_nb )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks decent and results could be improved with nlp techniques such as proper preprocessing, stop words removal\n",
    "or other algos. But this is okay for our experiements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For just one classifier with one vectorizer, the performance of GridSearch and RandomSearch are nearly the same.\n",
    "\n",
    "### But there are many other classifiers with their own parameters ?\n",
    "### We could even use a different vectorizer ( count, tf-idf, hashing with its own parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE ANSWER IS BUILDING A PIPELINE AND DO A GRIDSEARCH ON THEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple example pipeline to to a classifier, its parameters, vectorizer and GridSearchCV\n",
    "##### lets do this again with 4 categories for time saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2 categories and get train and test data\n",
    "\n",
    "categories = ['alt.atheism', 'rec.sport.baseball']\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True, \n",
    "                                  remove=('headers', 'footers', 'quotes'), categories= categories)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True, remove=('headers', 'footers', 'quotes'),\n",
    "                                 categories=categories)\n",
    "\n",
    "y_train = twenty_train.target\n",
    "y_test = twenty_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets initialize a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DummyEstimator(BaseEstimator):\n",
    "    def fit(self): pass\n",
    "    def score(self): pass\n",
    "\n",
    "class DummyTransformer(TransformerMixin):\n",
    "    def fit_transform(self, X, y=None, **fit_params): pass\n",
    "    def fit(self): pass\n",
    "    def tranform(self): pass\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', DummyEstimator()),\n",
    "])\n",
    "\n",
    "# A fit transform followed by an estimator\n",
    "\n",
    "search_space = [\n",
    "    {\n",
    "        'vect': (TfidfVectorizer(),),\n",
    "        'vect__max_df': (0.5, 0.75),\n",
    "        'clf': (LogisticRegression(),),\n",
    "        'clf__C': ( 0.1, 1, 10),\n",
    "        'clf__penalty': ('l2', 'l1'),\n",
    "    }, \n",
    "    {\n",
    "        'vect': (TfidfVectorizer(),),\n",
    "        'vect__max_df': (0.5, 0.75),\n",
    "        'clf': (RandomForestClassifier(),),\n",
    "        'clf__n_estimators': (10,20,30,50),\n",
    "        'clf__max_depth': (5,10,20),\n",
    "    },\n",
    "    {\n",
    "        'vect': (TfidfVectorizer(),),\n",
    "        'vect__max_df': (0.5, 0.75),\n",
    "        'clf': (DecisionTreeClassifier(),),\n",
    "        'clf__max_depth': (1,5,10,25,50),\n",
    "    }, \n",
    "    {\n",
    "        'vect': (TfidfVectorizer(),),\n",
    "        'vect__max_df': (0.5, 0.75),\n",
    "        'clf': (MultinomialNB(),),\n",
    "        'clf__alpha': (0.01,0.1, 1.0, 10, 100),\n",
    "    }   \n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(pipeline, search_space, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 56 candidates, totalling 280 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/s2917623/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Done 280 out of 280 | elapsed:   52.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words...\n",
       "                                                   lowercase=True, max_df=0.5,\n",
       "                                                   max_features=None, min_df=1,\n",
       "                                                   ngram_range=(1, 1),\n",
       "                                                   norm='l2', preprocessor=None,\n",
       "                                                   smooth_idf=True,\n",
       "                                                   stop_words=None,\n",
       "                                                   strip_accents=None,\n",
       "                                                   sublinear_tf=False,\n",
       "                                                   token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                   tokenizer=None, use_idf=True,\n",
       "                                                   vocabulary=None),),\n",
       "                          'vect__max_df': (0.5, 0.75)}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " best estimator is {'clf': MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True), 'clf__alpha': 0.01, 'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=0.5, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None), 'vect__max_df': 0.5} and score is 0.9591457753017641\n"
     ]
    }
   ],
   "source": [
    "gs_clf.best_params_\n",
    "# {'clf__C': 0.1}\n",
    "\n",
    "gs_clf.best_score_\n",
    "# 0.702290076336\n",
    "\n",
    "print (f\" best estimator is {gs_clf.best_params_} and score is {gs_clf.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can add even more parameters and search your classifiers with even more vectorizers like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1665c64ab6b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m pipeline = Pipeline([\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'vect'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'clf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDummyEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m ])\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', DummyEstimator()),\n",
    "])\n",
    "\n",
    "# A fit transform followed by an estimator\n",
    "\n",
    "search_space = [\n",
    "    {\n",
    "        'vect': (TfidfVectorizer(),),\n",
    "        'vect__max_df': (0.5, 0.75),\n",
    "        'clf': (LogisticRegression(),),\n",
    "        'clf__C': ( 0.1, 1, 10),\n",
    "        'clf__penalty': ('l2', 'l1'),\n",
    "    }, \n",
    "    {\n",
    "        'vect': (TfidfVectorizer(),),\n",
    "        'vect__max_df': (0.5, 0.75),\n",
    "        'clf': (RandomForestClassifier(),),\n",
    "        'clf__n_estimators': (10,20,30,50),\n",
    "        'clf__max_depth': (5,10,20),\n",
    "    },\n",
    "    {\n",
    "        'vect': (TfidfVectorizer(),),\n",
    "        'vect__max_df': (0.5, 0.75),\n",
    "        'clf': (DecisionTreeClassifier(),),\n",
    "        'clf__max_depth': (1,5,10,25,50),\n",
    "    }, \n",
    "    {\n",
    "        'vect': (TfidfVectorizer(),),\n",
    "        'vect__max_df': (0.5, 0.75),\n",
    "        'clf': (MultinomialNB(),),\n",
    "        'clf__alpha': (0.01,0.1, 1.0, 10, 100),\n",
    "    },\n",
    "        {\n",
    "        'vect': (CountVectorizer(),),\n",
    "        'vect__max_df': (0.5, 0.75),\n",
    "        'clf': (LogisticRegression(),),\n",
    "        'clf__C': ( 0.1, 1, 10),\n",
    "        'clf__penalty': ('l2', 'l1'),\n",
    "    }, \n",
    "    {\n",
    "        'vect': (CountVectorizer(),),\n",
    "        'vect__max_df': (0.5, 0.75),\n",
    "        'clf': (RandomForestClassifier(),),\n",
    "        'clf__n_estimators': (10,20,30,50),\n",
    "        'clf__max_depth': (5,10,20),\n",
    "    },\n",
    "    {\n",
    "        'vect': (CountVectorizer(),),\n",
    "        'vect__max_df': (0.5, 0.75),\n",
    "        'clf': (DecisionTreeClassifier(),),\n",
    "        'clf__max_depth': (1,5,10,25,50),\n",
    "    }, \n",
    "    {\n",
    "        'vect': (CountVectorizer(),),\n",
    "        'vect__max_df': (0.5, 0.75),\n",
    "        'clf': (MultinomialNB(),),\n",
    "        'clf__alpha': (0.01,0.1, 1.0, 10, 100),\n",
    "    }\n",
    "    \n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 112 candidates, totalling 560 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/s2917623/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/s2917623/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " best estimator is {'clf': MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True), 'clf__alpha': 0.01, 'vect': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='content', lowercase=True, max_df=0.5, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
      "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, use_idf=True, vocabulary=None), 'vect__max_df': 0.5} and score is 0.9591457753017641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 560 out of 560 | elapsed:  1.6min finished\n"
     ]
    }
   ],
   "source": [
    "gs_clf = GridSearchCV(pipeline, search_space, cv=5, verbose=1)\n",
    "gs_clf.fit(twenty_train.data, twenty_train.target)\n",
    "print (f\" best estimator is {gs_clf.best_params_} and score is {gs_clf.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next More into pipelines, base estimator and transformer classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before that Practise pipeline with GridSearch on iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
