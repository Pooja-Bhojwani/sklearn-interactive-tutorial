{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whats Sklearn or Scikit learn\n",
    "\n",
    "### Machine Learning in Python. Simple and efficient tools for data mining and data analysis\n",
    "\n",
    "### We will learn the following using scikit learn \n",
    "\n",
    "\n",
    "\n",
    "* Training Supervised Algorithms Linear regression, Random Forest etc\n",
    "* Training Unsupervised Algorithms : Clustering\n",
    "* hyper paramaters tuning with Gridsearch and RandomSearch\n",
    "* pipelines, transformers and estimators\n",
    "* Dealing with Imbalance datasets using SMOTE\n",
    "* Metrics in Scikit Learn\n",
    "* Some Assignments\n",
    "* A kaggle project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "In supervised learning, you train a machine learning model with the help of labelled data. Once a model has been trained using a input feature and labels, the model can predict the labels of test data. \n",
    "\n",
    "### Examples of Algorithms \n",
    "\n",
    "* Linear Regression: a Machine Learning algorithm that allows us to map numeric inputs to numeric outputs, fitting a line into the data points.\n",
    "\n",
    "  Link : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "  \n",
    "  Check out this example:\n",
    "  https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py\n",
    "\n",
    "* Logistic Regression: a classification algorithm that is widely used when the dependent variable is binary (0 or 1).\n",
    "\n",
    "  Link: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "  \n",
    "  Checkout this example:\n",
    "  https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic.html#sphx-glr-auto-examples-linear-model-plot-logistic-py\n",
    "\n",
    "* Support Vector Machines: a Machine Learning algorithm that uses Margin Maximization in determining the optimal separator line between classes, utilizing the Kernel Trick.\n",
    "\n",
    "  Link: https://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "* Decision Trees: Tree/ Graph algorithm based on simple rules. It creates a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "\n",
    "  Link : https://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "\n",
    "* Random Forest: Random Forest is an Ensemble model where a large number of weak classifiers (In this case smaller decison trees) are trained to model the data distribution. This method of training a number of smaller classifier to learn a function is called Bagging or bootstrap aggregation. Bagging is a special case of the model averaging approach. \n",
    "\n",
    "  Link : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "  Read more about Random Forests here : https://towardsdatascience.com/understanding-random-forest-58381e0602d2\n",
    "  \n",
    "  Difference between bagging and boosting: https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn (scikit-learn) -The Most popular library for non deep learning based machine learning in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import display, Math, Latex\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit Learn Datasets\n",
    "\n",
    "Scikit learn provides us with a large number of curated datasets to help us learn and use the various machine learning\n",
    "algorithms that it supports. This include both quantitative, text and images\n",
    "\n",
    "Link : https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Linear models includes the following\n",
    "\n",
    "* Oridinary Linear regression\n",
    "* Lasso Regression - L1 Penalty\n",
    "* Rige Regression - L2 Penalty\n",
    "* Logistic Regression \n",
    "\n",
    "\n",
    "### We will begin with Linear regression on the Boston Housing data set\n",
    "\n",
    "$$ \\text{min}_{w, b} \\sum_i || w^\\mathsf{T}x_i + b  - y_i||^2 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston # This method will download and load the boston datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston=load_boston()\n",
    "print(type(boston))\n",
    "boston.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]\n",
      " ...\n",
      " [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]\n",
      " [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]\n",
      " [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(boston.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "print(boston.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Want to see what those features mean ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets learn more about this dataset. Data Exploration is sometimes key in understanding your data\n",
    "\n",
    "We will convert the data to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boston = pd.DataFrame(boston.data)\n",
    "df_boston.columns = boston.feature_names\n",
    "df_boston['PRICE'] = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  PRICE  \n",
       "0     15.3  396.90   4.98   24.0  \n",
       "1     17.8  396.90   9.14   21.6  \n",
       "2     17.8  392.83   4.03   34.7  \n",
       "3     18.7  394.63   2.94   33.4  \n",
       "4     18.7  396.90   5.33   36.2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The target is the housing prices\n",
    "boston.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    506.000000\n",
       "mean      68.574901\n",
       "std       28.148861\n",
       "min        2.900000\n",
       "25%       45.025000\n",
       "50%       77.500000\n",
       "75%       94.075000\n",
       "max      100.000000\n",
       "Name: AGE, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets study one of the features - AGE\n",
    "df_boston.AGE.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10c6a58d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "sns.boxplot(x=\"AGE\",\n",
    "              data=df_boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    506.000000\n",
       "mean      22.532806\n",
       "std        9.197104\n",
       "min        5.000000\n",
       "25%       17.025000\n",
       "50%       21.200000\n",
       "75%       25.000000\n",
       "max       50.000000\n",
       "Name: PRICE, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Always understand what we are predicting. We will learn to work with missing values etc later\n",
    "\n",
    "sns.boxplot(x=\"PRICE\",\n",
    "              data=df_boston)\n",
    "df_boston['PRICE'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that there are outliers in the data, too many standard deviations away from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression # Loads the linear regression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the input data and labels\n",
    "\n",
    " # drops the prices column in the datasets. We will predict the prices or use it as our label/target\n",
    "X = df_boston.drop('PRICE', axis=1).values\n",
    "y = df_boston['PRICE'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, we need to split our data into train and test set. Again we will use Sklearn tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5, test_size=0.3) \n",
    "\n",
    "# Above we split our total data into 70% train and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets fit our training data using a linear regression model\n",
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, y_train) #training the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([37.38997666, 31.56794193, 27.13373886,  6.55117625, 33.69310848,\n",
       "        5.54919368, 27.10005629, 29.82980986, 26.44622421, 22.38873525])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have trained model, lets make predictions on the test data\n",
    "y_pred=regressor.predict(X_test)[:10]\n",
    "\n",
    "# The below are the predicted prices\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 on training set: 0.761160\n",
      "R^2 on test set: 0.677170\n"
     ]
    }
   ],
   "source": [
    "# The regression library comes with a score function\n",
    "print(\"R^2 on training set: %f\" % regressor.score(X_train, y_train))\n",
    "print(\"R^2 on test set: %f\" % regressor.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Always check the sklearn parameters for each algorithm\n",
    "Parameters:\t\n",
    "fit_intercept : boolean, optional, default True\n",
    "whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (e.g. data is expected to be already centered).\n",
    "\n",
    "normalize : boolean, optional, default False\n",
    "This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit on an estimator with normalize=False.\n",
    "\n",
    "copy_X : boolean, optional, default True\n",
    "If True, X will be copied; else, it may be overwritten.\n",
    "\n",
    "n_jobs : int or None, optional (default=None)\n",
    "The number of jobs to use for the computation.None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details.\n",
    "\n",
    "Attributes:\t\n",
    "coef_ : array, shape (n_features, ) or (n_targets, n_features)\n",
    "Estimated coefficients for the linear regression problem. \n",
    "\n",
    "intercept_ : array\n",
    "Independent term in the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 on training set: 0.761160\n",
      "R^2 on test set: 0.677170\n"
     ]
    }
   ],
   "source": [
    "\n",
    "regressor = LinearRegression(copy_X=True, fit_intercept=True, normalize=False) # There are always more parameters\n",
    "regressor.fit(X_train, y_train) #training the algorithm\n",
    "print(\"R^2 on training set: %f\" % regressor.score(X_train, y_train))\n",
    "print(\"R^2 on test set: %f\" % regressor.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall our input data. Its import to normalize our data to make better predictions. Sklearn provides many tools\n",
    "for data preprocessing. For example our linear regression model allows us to set normalize the data by setting\n",
    "#### normalize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression(copy_X=True, fit_intercept=True, normalize=True) # There are always more parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OR you could use the preprocessing library in Sklearn\n",
    "\n",
    "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n",
    "\n",
    "Mostly we use standard scalar or normalize functions. Basically the idea is to have the input features as a normal distribution with mean around zero and standard deviation of one (z-score).\n",
    "\n",
    "z = (x - mu)/std where mu is the mean and std is the standard deviation.\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    506.000000\n",
      "mean      22.532806\n",
      "std        9.197104\n",
      "min        5.000000\n",
      "25%       17.025000\n",
      "50%       21.200000\n",
      "75%       25.000000\n",
      "max       50.000000\n",
      "Name: PRICE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# We can even scale our target variable\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "X_train_norm = normalize(X_train) \n",
    "X_test_norm = normalize(X_test)\n",
    "\n",
    "# We can even do this for our target variable 'prices'\n",
    "print (df_boston['PRICE'].describe())\n",
    "y_train_norm = y_train/50  # dividing by the max\n",
    "y_test_norm = y_test/50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 on training set: 0.774417\n",
      "R^2 on test set: 0.662656\n"
     ]
    }
   ],
   "source": [
    "regressor = LinearRegression(copy_X=True, fit_intercept=True, normalize=False) # There are always more parameters\n",
    "regressor.fit(X_train_norm, y_train) #training the algorithm\n",
    "print(\"R^2 on training set: %f\" % regressor.score(X_train_norm, y_train))\n",
    "print(\"R^2 on test set: %f\" % regressor.score(X_test_norm, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simple dataset, we have no improvement in our performance, but you will see the importance of this as we move\n",
    "ahead this this class\n",
    "\n",
    "To understand more about importance of feature scaling, study the below links\n",
    "\n",
    "* https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py\n",
    "\n",
    "\n",
    "###### Recommended to check this out\n",
    "* https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that a simple Linear regression model with just 2 features is about fitting a straight line and higher the features we have , we fit a hyper plane instead. Lines have intercepts and coefficients.\n",
    "\n",
    "y = b0 + m1b1 + m2b2 + m3b3 + … … mnbn\n",
    "\n",
    "#### We can use the attributes of our trained model to see our coefficients and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3934743194948673"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIM</td>\n",
       "      <td>-131.846359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZN</td>\n",
       "      <td>15.549806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INDUS</td>\n",
       "      <td>8.560365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHAS</td>\n",
       "      <td>546.404225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NOX</td>\n",
       "      <td>-5139.190998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RM</td>\n",
       "      <td>2850.804773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AGE</td>\n",
       "      <td>-6.954764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DIS</td>\n",
       "      <td>-541.893046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RAD</td>\n",
       "      <td>163.530146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TAX</td>\n",
       "      <td>6.566476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>PTRATIO</td>\n",
       "      <td>-416.957308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>B</td>\n",
       "      <td>14.095042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LSTAT</td>\n",
       "      <td>-196.964658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    feature        coeff\n",
       "0      CRIM  -131.846359\n",
       "1        ZN    15.549806\n",
       "2     INDUS     8.560365\n",
       "3      CHAS   546.404225\n",
       "4       NOX -5139.190998\n",
       "5        RM  2850.804773\n",
       "6       AGE    -6.954764\n",
       "7       DIS  -541.893046\n",
       "8       RAD   163.530146\n",
       "9       TAX     6.566476\n",
       "10  PTRATIO  -416.957308\n",
       "11        B    14.095042\n",
       "12    LSTAT  -196.964658"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = boston.feature_names\n",
    "coefficents =regressor.coef_ # these are the coeff for our 13 features\n",
    "df_boston_model=pd.DataFrame(features,\n",
    "                             columns= ['feature'])\n",
    "df_boston_model['coeff'] = coefficents\n",
    "df_boston_model.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ROOT MEAN SQUARE ERROR AS A MEASURE TO STUDY MODEL PERFORMANCE. One of the metrics supported by sklearn\n",
    "\n",
    "$$\n",
    "\\text{MSE}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples} - 1} (y_i - \\hat{y}_i)^2.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.068341850927226\n",
      "30.697037704088686\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "regressor = LinearRegression(copy_X=True, fit_intercept=True, normalize=False) # There are always more parameters\n",
    "regressor.fit(X_train, y_train) #training the algorithm\n",
    "\n",
    "y_test_pred = regressor.predict(X_test)\n",
    "y_train_pred = regressor.predict(X_train)\n",
    "print (mean_squared_error(y_train_pred, y_train))\n",
    "print (mean_squared_error(y_test_pred, y_test))\n",
    "\n",
    "# There is room for more improvement here, We will try with more complex models later to get better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets not discuss about Lasso regressor and Ridge Regressor which uses L1 and L2 regularization functions respectively.\n",
    "\n",
    "\n",
    "* The Lasso estimator(L1 Penalty)\n",
    "The Lasso estimator is useful to impose sparsity on the coefficient. In other words, it is to be prefered if we believe that many of the features are not relevant. This is done via the so-called l1 penalty.\n",
    "\n",
    "$$ \\text{min}_{w, b} \\sum_i \\frac{1}{2} || w^\\mathsf{T}x_i + b  - y_i||^2  + \\alpha ||w||_1$$\n",
    "\n",
    "Link : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "\n",
    "* Ridge Regression (L2 penalty)¶\n",
    "The Ridge estimator is a simple regularization (called l2 penalty) of the ordinary LinearRegression. In particular, it has the benefit of being not computationally more expensive than the ordinary least square estimate.\n",
    "\n",
    "$$ \\text{min}_{w,b}  \\sum_i || w^\\mathsf{T}x_i + b  - y_i||^2  + \\alpha ||w||_2^2$$\n",
    "The amount of regularization is set via the alpha parameter of the Ridge.\n",
    "\n",
    "Link : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "### Readmore about L1 and L2\n",
    "Difference between L1 and L2 regularization : http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 on training set: 0.761155\n",
      "R^2 on test set: 0.676645\n"
     ]
    }
   ],
   "source": [
    "clf_lasso = Lasso(alpha=0.001)\n",
    "clf_lasso.fit(X_train, y_train)\n",
    "print(\"R^2 on training set: %f\" % clf_lasso.score(X_train, y_train))\n",
    "print(\"R^2 on test set: %f\" % clf_lasso.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 on training set: 0.761160\n",
      "R^2 on test set: 0.677156\n"
     ]
    }
   ],
   "source": [
    "clf_ridge = Ridge(alpha=0.001)\n",
    "clf_ridge.fit(X_train, y_train)\n",
    "print(\"R^2 on training set: %f\" % clf_ridge.score(X_train, y_train))\n",
    "print(\"R^2 on test set: %f\" % clf_ridge.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q. Try different values of alpha and check the change in score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IF you notice above alpha or the regularization coefficient is a hyper parameter. This needs to be tuned to get the best possible results. Hyper tuning and cross validation using sklearn will be explained in the later slides.\n",
    "\n",
    "# However check the below example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "The value for regularization is: 0.001\n",
      "R^2 on training set: 0.761160\n",
      "R^2 on test set: 0.677156\n",
      "==========\n",
      "The value for regularization is: 0.01\n",
      "R^2 on training set: 0.761159\n",
      "R^2 on test set: 0.677033\n",
      "==========\n",
      "The value for regularization is: 0.1\n",
      "R^2 on training set: 0.761111\n",
      "R^2 on test set: 0.675869\n",
      "==========\n",
      "The value for regularization is: 1\n",
      "R^2 on training set: 0.759671\n",
      "R^2 on test set: 0.668636\n",
      "==========\n",
      "The value for regularization is: 10\n",
      "R^2 on training set: 0.755695\n",
      "R^2 on test set: 0.658493\n",
      "==========\n",
      "The value for regularization is: 100\n",
      "R^2 on training set: 0.740130\n",
      "R^2 on test set: 0.645040\n"
     ]
    }
   ],
   "source": [
    "for value in [0.001, 0.01, 0.1, 1, 10, 100]:\n",
    "    print (\"=\"* 10)\n",
    "    print (f\"The value for regularization is: {value}\")\n",
    "    clf_ridge = Ridge(alpha=value)\n",
    "    clf_ridge.fit(X_train, y_train)\n",
    "    print(\"R^2 on training set: %f\" % clf_ridge.score(X_train, y_train))\n",
    "    print(\"R^2 on test set: %f\" % clf_ridge.score(X_test, y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity Time\n",
    "Try out different values of alpha for Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GridSearch, Random search of hyper parameters and Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "1. https://towardsdatascience.com/what-are-supervised-and-unsupervised-learning-in-machine-learning-dc76bd67795d\n",
    "2. https://bigdata-madesimple.com/how-to-run-linear-regression-in-python-scikit-learn/\n",
    "3. https://github.com/amueller/scipy-2018-sklearn/blob/master/notebooks/17.In_Depth-Linear_Models.ipynb\n",
    "4. Sklearn\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
